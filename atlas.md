# ğŸ›°ï¸ A.T.L.A.S.
## Anomaly Tracking & Logistics Analytic Segmentation

### Projektbeschreibung
Supply-Chain-Management-System mit ML-gestÃ¼tzter Anomalieerkennung
und Lieferanten-Clustering. Das System erkennt automatisch ungewÃ¶hnliche
LagerbestÃ¤nde (Isolation Forest) und gruppiert Lieferanten nach
Leistungsprofil (K-Means Clustering).

### Technologie-Stack
| Kategorie | Technologie |
|---|---|
| **Backend** | FastAPI + SQLAlchemy + SQLite |
| **ML** | scikit-learn (Isolation Forest, K-Means) |
| **Frontend** | Streamlit + Plotly |
| **Architektur** | MVC Pattern |
| **Sprache** | Python 3.11+ |

### Architektur (MVC)
```
View (Streamlit)  â†’  Controller (FastAPI)  â†’  Model/Service (DB + ML)
  dashboard.py        endpoints/*.py           orm_models.py
  Nur anzeigen        Vermittelt               anomaly_detection.py
  Port 8501           Port 8000                clustering.py
```

### Projektstruktur
```
ATLAS/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ main.py                  FastAPI App + Router
â”‚   â”‚   â”œâ”€â”€ dependencies.py          Session-Verwaltung (DI)
â”‚   â”‚   â””â”€â”€ endpoints/
â”‚   â”‚       â”œâ”€â”€ inventory.py         Inventory CRUD Endpunkte
â”‚   â”‚       â”œâ”€â”€ suppliers.py         Supplier CRUD Endpunkte
â”‚   â”‚       â””â”€â”€ ml.py               ML-Ergebnis Endpunkte
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ database.py              SQLite + SQLAlchemy Engine
â”‚   â”‚   â”œâ”€â”€ orm_models.py            Inventory + Supplier Tabellen
â”‚   â”‚   â””â”€â”€ schemas.py              Pydantic Validierung
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ anomaly_detection.py     Isolation Forest Service
â”‚   â”‚   â””â”€â”€ clustering.py           K-Means Clustering Service
â”‚   â””â”€â”€ frontend/
â”‚       â”œâ”€â”€ dashboard.py            Streamlit Dashboard
â”‚       â””â”€â”€ assets/
â”‚           â””â”€â”€ atlas_bg.png        Hintergrundbild
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ seed_data.py                Testdaten-Generator
â”‚   â””â”€â”€ start.py                    Ein-Klick-Startskript
â”œâ”€â”€ atlas.md                        Projektdokumentation
â””â”€â”€ requirements.txt                Python-Dependencies
```

### API-Endpunkte
| Methode | Route | Beschreibung |
|---|---|---|
| GET | `/api/inventory/` | Alle Produkte abrufen |
| POST | `/api/inventory/` | Neues Produkt anlegen |
| GET | `/api/inventory/{id}` | Einzelnes Produkt abrufen |
| GET | `/api/suppliers/` | Alle Lieferanten abrufen |
| POST | `/api/suppliers/` | Neuen Lieferanten anlegen |
| GET | `/api/suppliers/{id}` | Einzelnen Lieferanten abrufen |
| GET | `/api/ml/anomalies` | Isolation Forest ausfÃ¼hren |
| GET | `/api/ml/clusters` | K-Means Clustering ausfÃ¼hren |

### ML-Modelle

#### Isolation Forest (Anomalieerkennung)
- **Zweck:** Erkennt ungewÃ¶hnliche LagerbestÃ¤nde
- **Features:** `quantity`, `price`, `reorder_level`
- **Methode:** Unsupervised Learning â€” findet AusreiÃŸer ohne Labels
- **Konfiguration:** contamination=0.05, random_state=42
- **Ergebnis:** ~5% der Produkte als Anomalien markiert

#### K-Means Clustering (Lieferantenbewertung)
- **Zweck:** Gruppiert Lieferanten nach Leistungsprofil
- **Features:** `delivery_reliability`, `avg_delivery_days`, `price_level`, `quality_score`
- **Methode:** Unsupervised Learning â€” findet natÃ¼rliche Gruppen
- **Konfiguration:** n_clusters=3, n_init=10, random_state=42
- **Ergebnis:** 3 Cluster (Premium, Standard, Risiko)

### Testdaten
- **500 Produkte** in 5 Kategorien (Befestigung, Elektronik, Werkzeug, Hydraulik, Verpackung)
- **50 Lieferanten** mit realistischen Leistungsdaten
- **5% bewusste Anomalien** (extreme Mengen/Preise) zum Testen

### Dashboard-Seiten
| Seite | Inhalt |
|---|---|
| **Dashboard** | Ãœbersicht: Anzahl Produkte, Lieferanten, Kategorien |
| **Anomalieerkennung** | Scatter-Plot (Quantity vs. Price), Anomalie-Tabelle |
| **Lieferanten-Cluster** | Cluster-Expander mit Statistiken, zwei Scatter-Plots |

### Schnellstart
```bash
# 1. Repository klonen und venv erstellen
cd ATLAS
python -m venv venv
venv\Scripts\activate

# 2. Dependencies installieren
pip install -r requirements.txt

# 3. Datenbank befÃ¼llen
py scripts/seed_data.py

# 4. Alles starten
py scripts/start.py

# 5. Im Browser Ã¶ffnen
# Dashboard: http://localhost:8501
# API Docs:  http://localhost:8000/docs
```

### Geplante Features (Phase 2)
- [ ] **MLOps-Lifecycle**
  - Modelle speichern/laden mit joblib
  - Model Monitoring (Silhouette Score, Precision/Recall)
  - Schwellwert-basiertes Re-Training
  - API-Endpunkt: `/api/ml/retrain`
  - Zentrale Modellverwaltung (`model_manager.py`)
- [ ] **Docker Deployment**
  - Dockerfile + docker-compose.yml
  - SQLite â†’ PostgreSQL Migration
  - Umgebungsvariablen fÃ¼r Konfiguration
- [ ] **Erweiterte ML-Features**
  - Feature Engineering (z.B. Kapitalbindung = quantity Ã— price)
  - Business Rules + ML kombiniert
  - Automatische Cluster-Benennung
- [ ] **Testing**
  - Unit Tests fÃ¼r Services
  - Integration Tests fÃ¼r API-Endpunkte
  - ML-Model Validierung